import os
import gradio as gr
from dotenv import load_dotenv
from langchain.document_loaders import TextLoader
from rag_demo import load_and_split_document, create_vector_store, setup_rag_chain, execute_query, get_retriever

# ç¯å¢ƒè®¾ç½®
load_dotenv()  # åŠ è½½ç¯å¢ƒå˜é‡
QUESTION_LANG = os.getenv("QUESTION_LANG")  # ä»ç¯å¢ƒå˜é‡è·å– QUESTION_LANG
assert QUESTION_LANG in ['cn', 'en'], QUESTION_LANG

if QUESTION_LANG == "cn":
    title = "ZeroPal"
    title_markdown = """
    <div align="center">
        <img src="https://raw.githubusercontent.com/puyuan1996/RAG/main/assets/banner.svg" width="80%" height="20%" alt="Banner Image">
    </div>
    
    ğŸ“¢ **æ“ä½œè¯´æ˜**ï¼šè¯·åœ¨ä¸‹æ–¹çš„â€œé—®é¢˜â€æ¡†ä¸­è¾“å…¥å…³äº LightZero çš„é—®é¢˜ï¼Œå¹¶ç‚¹å‡»â€œæäº¤â€æŒ‰é’®ã€‚å³ä¾§çš„â€œå›ç­”â€æ¡†å°†å±•ç¤º RAG æ¨¡å‹æä¾›çš„ç­”æ¡ˆã€‚
    æ‚¨å¯ä»¥åœ¨é—®ç­”æ¡†ä¸‹æ–¹æŸ¥çœ‹å½“å‰â€œå¯¹è¯å†å²â€ï¼Œç‚¹å‡»â€œæ¸…é™¤ä¸Šä¸‹æ–‡â€æŒ‰é’®å¯æ¸…ç©ºå†å²è®°å½•ã€‚åœ¨â€œå¯¹è¯å†å²â€æ¡†ä¸‹æ–¹ï¼Œæ‚¨å°†æ‰¾åˆ°ç›¸å…³å‚è€ƒæ–‡æ¡£ï¼Œå…¶ä¸­ç›¸å…³æ–‡æ®µå°†ä»¥é»„è‰²é«˜äº®æ˜¾ç¤ºã€‚
    å¦‚æœæ‚¨å–œæ¬¢è¿™ä¸ªé¡¹ç›®ï¼Œè¯·åœ¨ GitHub [LightZero RAG Demo](https://github.com/puyuan1996/RAG) ä¸Šç»™æˆ‘ä»¬ç‚¹èµï¼âœ¨ æ‚¨çš„æ”¯æŒæ˜¯æˆ‘ä»¬æŒç»­æ›´æ–°çš„åŠ¨åŠ›ã€‚
    
    <div align="center">
        <strong>æ³¨æ„ï¼šç®—æ³•æ¨¡å‹è¾“å‡ºå¯èƒ½åŒ…å«ä¸€å®šçš„éšæœºæ€§ã€‚ç»“æœä¸ä»£è¡¨å¼€å‘è€…å’Œç›¸å…³ AI æœåŠ¡çš„æ€åº¦å’Œæ„è§ã€‚æœ¬é¡¹ç›®å¼€å‘è€…ä¸å¯¹ç»“æœä½œå‡ºä»»ä½•ä¿è¯ï¼Œä»…ä¾›å‚è€ƒä¹‹ç”¨ã€‚ä½¿ç”¨è¯¥æœåŠ¡å³ä»£è¡¨åŒæ„åæ–‡æ‰€è¿°çš„ä½¿ç”¨æ¡æ¬¾ã€‚</strong>
    </div>
    """
    tos_markdown = """
    ### ä½¿ç”¨æ¡æ¬¾
    
    ä½¿ç”¨æœ¬æœåŠ¡çš„ç©å®¶éœ€åŒæ„ä»¥ä¸‹æ¡æ¬¾ï¼š
    
    - æœ¬æœåŠ¡ä¸ºæ¢ç´¢æ€§ç ”ç©¶çš„é¢„è§ˆç‰ˆï¼Œä»…ä¾›éå•†ä¸šç”¨é€”ã€‚
    - æœåŠ¡ä¸å¾—ç”¨äºä»»ä½•éæ³•ã€æœ‰å®³ã€æš´åŠ›ã€ç§æ—ä¸»ä¹‰æˆ–å…¶ä»–ä»¤äººåæ„Ÿçš„ç›®çš„ã€‚
    - æœåŠ¡æä¾›æœ‰é™çš„å®‰å…¨æªæ–½ï¼Œå¹¶å¯èƒ½ç”Ÿæˆä»¤äººåæ„Ÿçš„å†…å®¹ã€‚
    - å¦‚æœæ‚¨å¯¹æœåŠ¡ä½“éªŒä¸æ»¡ï¼Œè¯·é€šè¿‡ opendilab@pjlab.org.cn ä¸æˆ‘ä»¬è”ç³»ï¼æˆ‘ä»¬æ‰¿è¯ºä¿®å¤é—®é¢˜å¹¶ä¸æ–­æ”¹è¿›é¡¹ç›®ã€‚
    - ä¸ºäº†è·å¾—æœ€ä½³ä½“éªŒï¼Œè¯·ä½¿ç”¨å°å¼ç”µè„‘ï¼Œå› ä¸ºç§»åŠ¨è®¾å¤‡å¯èƒ½ä¼šå½±å“è§†è§‰æ•ˆæœã€‚
    
    **ç‰ˆæƒæ‰€æœ‰ Â© 2024 OpenDILabã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚**
    """

# è·¯å¾„å˜é‡ï¼Œæ–¹ä¾¿ä¹‹åçš„æ–‡ä»¶ä½¿ç”¨
file_path = './documents/LightZero_README_zh.md'

# åŠ è½½åŸå§‹Markdownæ–‡æ¡£
loader = TextLoader(file_path)
orig_documents = loader.load()

# å­˜å‚¨å¯¹è¯å†å²
conversation_history = []

chunks = load_and_split_document(file_path, chunk_size=5000, chunk_overlap=500)
vectorstore = create_vector_store(chunks, model='OpenAI')


def rag_answer(question, temperature, k):
    """
    å¤„ç†ç”¨æˆ·é—®é¢˜å¹¶è¿”å›ç­”æ¡ˆå’Œé«˜äº®æ˜¾ç¤ºçš„ä¸Šä¸‹æ–‡

    :param question: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
    :param temperature: ç”Ÿæˆç­”æ¡ˆæ—¶ä½¿ç”¨çš„æ¸©åº¦å‚æ•°
    :param k: æ£€ç´¢åˆ°çš„æ–‡æ¡£å—æ•°é‡
    :return: æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆå’Œé«˜äº®æ˜¾ç¤ºä¸Šä¸‹æ–‡çš„Markdownæ–‡æœ¬
    """
    try:
        retriever = get_retriever(vectorstore, k)
        rag_chain = setup_rag_chain(model_name='kimi', temperature=temperature)

        # å°†é—®é¢˜æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
        conversation_history.append(("User", question))

        # å°†å¯¹è¯å†å²è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        history_str = "\n".join([f"{role}: {text}" for role, text in conversation_history])

        retrieved_documents, answer = execute_query(retriever, rag_chain, history_str, model_name='kimi',
                                                    temperature=temperature)

        # åœ¨æ–‡æ¡£ä¸­é«˜äº®æ˜¾ç¤ºä¸Šä¸‹æ–‡
        context = [retrieved_documents[i].page_content for i in range(len(retrieved_documents))]
        highlighted_document = orig_documents[0].page_content
        for i in range(len(context)):
            highlighted_document = highlighted_document.replace(context[i], f"<mark>{context[i]}</mark>")

        # å°†å›ç­”æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
        conversation_history.append(("Assistant", answer))

        # å°†å¯¹è¯å†å²å­˜å‚¨åˆ°æ•°æ®åº“ä¸­ï¼ˆæ­¤å¤„çœç•¥æ•°æ®åº“æ“ä½œä»£ç ï¼‰

        # è¿”å›å®Œæ•´çš„å¯¹è¯å†å²
        full_history = "\n".join([f"{role}: {text}" for role, text in conversation_history])

    except Exception as e:
        print(f"An error occurred: {e}")
        return "å¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚", "", ""

    return answer, highlighted_document, full_history


def clear_context():
    """
    æ¸…é™¤å¯¹è¯å†å²
    """
    global conversation_history
    conversation_history = []
    return "", "", ""


def export_history():
    """
    å¯¼å‡ºå¯¹è¯å†å²è®°å½•
    """
    # ä»æ•°æ®åº“ä¸­è·å–å®Œæ•´çš„å¯¹è¯å†å²è®°å½•ï¼ˆæ­¤å¤„çœç•¥æ•°æ®åº“æ“ä½œä»£ç ï¼‰
    exported_history = "å¯¹è¯å†å²è®°å½•ï¼š\n" + "\n".join([f"{role}: {text}" for role, text in conversation_history])
    return exported_history


if __name__ == "__main__":
    with gr.Blocks(title=title, theme='ParityError/Interstellar') as zero_pal:
        gr.Markdown(title_markdown)

        with gr.Row():
            with gr.Column():
                inputs = gr.Textbox(
                    placeholder="è¯·æ‚¨åœ¨è¿™é‡Œè¾“å…¥ä»»ä½•å…³äº LightZero çš„é—®é¢˜ã€‚",
                    label="é—®é¢˜ (Q)")
                temperature = gr.Slider(minimum=0.0, maximum=1.0, value=0.01, step=0.01, label="æ¸©åº¦å‚æ•°")
                k = gr.Slider(minimum=1, maximum=10, value=5, step=1, label="æ£€ç´¢åˆ°çš„æ–‡æ¡£å—æ•°é‡")
                with gr.Row():
                    gr_submit = gr.Button('æäº¤')
                    gr_clear = gr.Button('æ¸…é™¤ä¸Šä¸‹æ–‡')

            outputs_answer = gr.Textbox(placeholder="å½“ä½ ç‚¹å‡»æäº¤æŒ‰é’®åï¼Œè¿™é‡Œä¼šæ˜¾ç¤º RAG æ¨¡å‹ç»™å‡ºçš„å›ç­”ã€‚",
                                        label="å›ç­” (A)")
        outputs_history = gr.Textbox(label="å¯¹è¯å†å²")
        with gr.Row():
            outputs_context = gr.Markdown(label="å‚è€ƒçš„æ–‡æ¡£ï¼Œæ£€ç´¢å¾—åˆ°çš„ context ç”¨é«˜äº®æ˜¾ç¤º (C)")
        gr_clear.click(clear_context, outputs=[outputs_context, outputs_history])
        gr_submit.click(
            rag_answer,
            inputs=[inputs, temperature, k],
            outputs=[outputs_answer, outputs_context, outputs_history],
        )
        gr.Markdown(tos_markdown)

    concurrency = int(os.environ.get('CONCURRENCY', os.cpu_count()))
    favicon_path = os.path.join(os.path.dirname(__file__), 'assets', 'avatar.png')
    zero_pal.queue().launch(max_threads=concurrency, favicon_path=favicon_path, share=True)
