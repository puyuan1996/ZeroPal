import collections
import os
import sqlite3
import threading

import gradio as gr
import numpy as np
from dotenv import load_dotenv
from langchain.document_loaders import TextLoader
from sentence_transformers import SentenceTransformer, util

from rag_demo import load_and_split_document, create_vector_store, setup_rag_chain, execute_query, get_retriever

# ç¯å¢ƒè®¾ç½®
load_dotenv()  # åŠ è½½ç¯å¢ƒå˜é‡
QUESTION_LANG = os.getenv("QUESTION_LANG")  # ä»ç¯å¢ƒå˜é‡è·å– QUESTION_LANG
assert QUESTION_LANG in ['cn', 'en'], QUESTION_LANG

if QUESTION_LANG == "cn":
    title = "ZeroPal"
    title_markdown = """
    <div align="center">
        <img src="https://raw.githubusercontent.com/puyuan1996/ZeroPal/main/assets/banner.svg" width="80%" height="20%" alt="Banner Image">
    </div>

    ğŸ“¢ **æ“ä½œè¯´æ˜**ï¼šè¯·åœ¨ä¸‹æ–¹çš„"é—®é¢˜"æ¡†ä¸­è¾“å…¥å…³äº LightZero çš„é—®é¢˜ï¼Œå¹¶ç‚¹å‡»"æäº¤"æŒ‰é’®ã€‚å³ä¾§çš„"å›ç­”"æ¡†å°†å±•ç¤º RAG æ¨¡å‹æä¾›çš„ç­”æ¡ˆã€‚
    æ‚¨å¯ä»¥åœ¨é—®ç­”æ¡†ä¸‹æ–¹æŸ¥çœ‹å½“å‰"å¯¹è¯å†å²"ï¼Œç‚¹å‡»"æ¸…é™¤å¯¹è¯å†å²"æŒ‰é’®å¯æ¸…ç©ºå†å²è®°å½•ã€‚åœ¨"å¯¹è¯å†å²"æ¡†ä¸‹æ–¹ï¼Œæ‚¨å°†æ‰¾åˆ°ç›¸å…³å‚è€ƒæ–‡æ¡£ï¼Œå…¶ä¸­ç›¸å…³æ–‡æ®µå°†ä»¥é»„è‰²é«˜äº®æ˜¾ç¤ºã€‚
    å¦‚æœæ‚¨å–œæ¬¢è¿™ä¸ªé¡¹ç›®ï¼Œè¯·åœ¨ GitHub [LightZero RAG Demo](https://github.com/puyuan1996/ZeroPal) ä¸Šç»™æˆ‘ä»¬ç‚¹èµï¼âœ¨ æ‚¨çš„æ”¯æŒæ˜¯æˆ‘ä»¬æŒç»­æ›´æ–°çš„åŠ¨åŠ›ã€‚æ³¨æ„ï¼šç®—æ³•æ¨¡å‹è¾“å‡ºå¯èƒ½åŒ…å«ä¸€å®šçš„éšæœºæ€§ã€‚ç»“æœä¸ä»£è¡¨å¼€å‘è€…å’Œç›¸å…³ AI æœåŠ¡çš„æ€åº¦å’Œæ„è§ã€‚æœ¬é¡¹ç›®å¼€å‘è€…ä¸å¯¹ç»“æœä½œå‡ºä»»ä½•ä¿è¯ï¼Œä»…ä¾›å‚è€ƒä¹‹ç”¨ã€‚ä½¿ç”¨è¯¥æœåŠ¡å³ä»£è¡¨åŒæ„åæ–‡æ‰€è¿°çš„ä½¿ç”¨æ¡æ¬¾ã€‚

    ğŸ“¢ **Instructions**: Please enter your questions about LightZero in the "Question" box below and click the "Submit" button. The "Answer" box on the right will display the answers provided by the RAG model.
    Below the Q&A box, you can view the current "Conversation History". Clicking the "Clear Conversation History" button will erase the history records. Below the "Conversation History" box, you'll find relevant reference documents, with the pertinent sections highlighted in yellow.
    If you like this project, please give us a thumbs up on GitHub at [LightZero RAG Demo](https://github.com/puyuan1996/ZeroPal)! âœ¨ Your support motivates us to keep updating.
    Note: The output from the algorithm model may contain a degree of randomness. The results do not represent the attitudes and opinions of the developers and related AI services. The developers of this project make no guarantees about the results, which are for reference only. Use of this service indicates agreement with the terms of use described later in the text.
    """
    tos_markdown = """
    ### ä½¿ç”¨æ¡æ¬¾

    ä½¿ç”¨æœ¬æœåŠ¡çš„ç©å®¶éœ€åŒæ„ä»¥ä¸‹æ¡æ¬¾ï¼š

    - æœ¬æœåŠ¡ä¸ºæ¢ç´¢æ€§ç ”ç©¶çš„é¢„è§ˆç‰ˆï¼Œä»…ä¾›éå•†ä¸šç”¨é€”ã€‚
    - æœåŠ¡ä¸å¾—ç”¨äºä»»ä½•éæ³•ã€æœ‰å®³ã€æš´åŠ›ã€ç§æ—ä¸»ä¹‰æˆ–å…¶ä»–ä»¤äººåæ„Ÿçš„ç›®çš„ã€‚
    - æœåŠ¡æä¾›æœ‰é™çš„å®‰å…¨æªæ–½ï¼Œå¹¶å¯èƒ½ç”Ÿæˆä»¤äººåæ„Ÿçš„å†…å®¹ã€‚
    - å¦‚æœæ‚¨å¯¹æœåŠ¡ä½“éªŒä¸æ»¡ï¼Œè¯·é€šè¿‡ opendilab@pjlab.org.cn ä¸æˆ‘ä»¬è”ç³»ï¼æˆ‘ä»¬æ‰¿è¯ºä¿®å¤é—®é¢˜å¹¶ä¸æ–­æ”¹è¿›é¡¹ç›®ã€‚
    - ä¸ºäº†è·å¾—æœ€ä½³ä½“éªŒï¼Œè¯·ä½¿ç”¨å°å¼ç”µè„‘ï¼Œå› ä¸ºç§»åŠ¨è®¾å¤‡å¯èƒ½ä¼šå½±å“è§†è§‰æ•ˆæœã€‚

    **ç‰ˆæƒæ‰€æœ‰ Â© 2024 OpenDILabã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚**
    """

# è·¯å¾„å˜é‡,æ–¹ä¾¿ä¹‹åçš„æ–‡ä»¶ä½¿ç”¨
file_path = './documents/LightZero_README_zh.md'

# åŠ è½½åŸå§‹Markdownæ–‡æ¡£
loader = TextLoader(file_path)
orig_documents = loader.load()

# å­˜å‚¨å¯¹è¯å†å²
conversation_history = {}

# åˆ›å»ºçº¿ç¨‹å±€éƒ¨æ•°æ®å¯¹è±¡
threadLocal = threading.local()


def get_db_connection():
    """
    è¿”å›å½“å‰çº¿ç¨‹çš„æ•°æ®åº“è¿æ¥
    """
    conn = getattr(threadLocal, 'conn', None)
    if conn is None:
        # åˆ›å»ºå­˜å‚¨å¯¹è¯å†å²çš„è¡¨
        conn = sqlite3.connect('database/conversation_history.db')
        c = conn.cursor()
        c.execute('''CREATE TABLE IF NOT EXISTS history
                     (id INTEGER PRIMARY KEY AUTOINCREMENT,
                     user_id TEXT NOT NULL,
                     user_input TEXT NOT NULL,
                     user_input_embedding BLOB NOT NULL,
                     assistant_output TEXT NOT NULL,
                     timestamp DATETIME DEFAULT CURRENT_TIMESTAMP)''')
        threadLocal.conn = conn
    return conn


def get_db_cursor():
    """
    è¿”å›å½“å‰çº¿ç¨‹çš„æ•°æ®åº“æ¸¸æ ‡
    """
    conn = get_db_connection()
    c = getattr(threadLocal, 'cursor', None)
    if c is None:
        c = conn.cursor()
        threadLocal.cursor = c
    return c


# ç¨‹åºç»“æŸæ—¶æ¸…ç†æ•°æ®åº“è¿æ¥
def close_db_connection():
    conn = getattr(threadLocal, 'conn', None)
    if conn is not None:
        conn.close()
        setattr(threadLocal, 'conn', None)

    c = getattr(threadLocal, 'cursor', None)
    if c is not None:
        c.close()
        setattr(threadLocal, 'cursor', None)


chunks = load_and_split_document(file_path, chunk_size=5000, chunk_overlap=500)
vectorstore = create_vector_store(chunks, model='OpenAI')

# åŠ è½½é¢„è®­ç»ƒçš„SBERTæ¨¡å‹
sbert_model = SentenceTransformer('all-MiniLM-L6-v2')

# å®šä¹‰ä½™å¼¦ç›¸ä¼¼åº¦é˜ˆå€¼
cosine_threshold = 0.96  # ä¸ºäº†æé«˜æ£€ç´¢çš„å‡†ç¡®æ€§ï¼Œå°†ä½™å¼¦ç›¸ä¼¼åº¦é˜ˆå€¼è°ƒé«˜

# è®¾ç½®LRUç¼“å­˜çš„å¤§å°
CACHE_SIZE = 1000

# åˆ›å»ºå†å²é—®é¢˜çš„ç¼“å­˜
conversation_history_cache = collections.OrderedDict()


# def rag_answer(question, temperature=0.01, k=5, user_id='user'):
def rag_answer(question, k=5, user_id='user'):
    """
    å¤„ç†ç”¨æˆ·é—®é¢˜å¹¶è¿”å›ç­”æ¡ˆå’Œé«˜äº®æ˜¾ç¤ºçš„ä¸Šä¸‹æ–‡

    :param question: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
    :param temperature: ç”Ÿæˆç­”æ¡ˆæ—¶ä½¿ç”¨çš„æ¸©åº¦å‚æ•°
    :param k: æ£€ç´¢åˆ°çš„æ–‡æ¡£å—æ•°é‡
    :param user_id: ç”¨æˆ·ID
    :return: æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆå’Œé«˜äº®æ˜¾ç¤ºä¸Šä¸‹æ–‡çš„Markdownæ–‡æœ¬
    """
    temperature = 0.01  # TODO: ä½¿ç”¨å›ºå®šçš„æ¸©åº¦å‚æ•°

    try:
        # è·å–å½“å‰çº¿ç¨‹çš„æ•°æ®åº“è¿æ¥å’Œæ¸¸æ ‡
        conn = get_db_connection()
        c = get_db_cursor()

        question_embedding = sbert_model.encode(question)
        question_embedding_bytes = question_embedding.tobytes()  # å°†numpyæ•°ç»„è½¬æ¢ä¸ºå­—èŠ‚ä¸²

        # ä»æ•°æ®åº“ä¸­è·å–æ‰€æœ‰ç”¨æˆ·çš„å¯¹è¯å†å²
        c.execute("SELECT user_input, user_input_embedding, assistant_output FROM history")
        all_history = c.fetchall()
        # åˆå§‹åŒ–æœ€é«˜çš„ä½™å¼¦ç›¸ä¼¼åº¦å’Œå¯¹åº”çš„ç­”æ¡ˆ
        max_cosine_score = 0
        best_answer = ""
        # åœ¨å†å²é—®é¢˜çš„ç¼“å­˜ä¸­æŸ¥æ‰¾ç›¸ä¼¼é—®é¢˜
        for history_question_bytes, (history_question, history_answer) in conversation_history_cache.items():
            history_question_embedding_numpy = np.frombuffer(history_question_bytes, dtype=np.float32)
            cosine_score = util.cos_sim(question_embedding, history_question_embedding_numpy).item()
            # print(f"æ£€ç´¢åˆ°å†å²é—®é¢˜: {history_question}")
            # print(f"å½“å‰é—®é¢˜ä¸å†å²é—®é¢˜çš„ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_score}")
            if cosine_score > cosine_threshold and cosine_score > max_cosine_score:
                max_cosine_score = cosine_score
                best_answer = history_answer

        if user_id not in conversation_history:
            conversation_history[user_id] = []

        conversation_history[user_id].append((f"User[{user_id}]", question))
        # å¦‚æœä½™å¼¦ç›¸ä¼¼åº¦é«˜äºé˜ˆå€¼,åˆ™æ›´æ–°æœ€ä½³ç­”æ¡ˆ
        if max_cosine_score > cosine_threshold:
            print('=' * 20)
            print(f"æ‰¾åˆ°äº†è¶³å¤Ÿç›¸ä¼¼çš„å†å²é—®é¢˜,ç›´æ¥è¿”å›å¯¹åº”çš„ç­”æ¡ˆã€‚ä½™å¼¦ç›¸ä¼¼åº¦ä¸º: {max_cosine_score}")
            answer = best_answer
        else:
            retriever = get_retriever(vectorstore, k)
            rag_chain = setup_rag_chain(model_name='kimi', temperature=temperature)
            history_str = "\n".join([f"{role}: {text}" for role, text in conversation_history[user_id]])
            history_question = [history_str, question]
            retrieved_documents, answer = execute_query(retriever, rag_chain, history_question, model_name='kimi',
                                                        temperature=temperature)

        # è·å–æ€»çš„å¯¹è¯è®°å½•æ•°
        c.execute("SELECT COUNT(*) FROM history")
        total_records = c.fetchone()[0]
        print(f"æ€»å¯¹è¯è®°å½•æ•°: {total_records}")

        # å°†é—®é¢˜å’Œå›ç­”å­˜å‚¨åˆ°æ•°æ®åº“
        c.execute(
            "INSERT INTO history (user_id, user_input, user_input_embedding, assistant_output) VALUES (?, ?, ?, ?)",
            (user_id, question, question_embedding_bytes, answer))
        conn.commit()

        # å°†æ–°é—®é¢˜å’Œç­”æ¡ˆæ·»åŠ åˆ°å†å²é—®é¢˜çš„ç¼“å­˜ä¸­
        conversation_history_cache[question_embedding_bytes] = (question, answer)
        # å¦‚æœç¼“å­˜å¤§å°è¶…è¿‡é™åˆ¶,åˆ™æ·˜æ±°æœ€è¿‘æœ€å°‘ä½¿ç”¨çš„é—®é¢˜
        if len(conversation_history_cache) > CACHE_SIZE:
            conversation_history_cache.popitem(last=False)

        if max_cosine_score > cosine_threshold:
            highlighted_document = ""
        else:
            # åœ¨æ–‡æ¡£ä¸­é«˜äº®æ˜¾ç¤ºä¸Šä¸‹æ–‡
            context = [retrieved_documents[i].page_content for i in range(len(retrieved_documents))]
            highlighted_document = orig_documents[0].page_content
            for i in range(len(context)):
                highlighted_document = highlighted_document.replace(context[i], f"<mark>{context[i]}</mark>")

        conversation_history[user_id].append(("Assistant", answer))
        full_history = "\n".join([f"{role}: {text}" for role, text in conversation_history[user_id]])

    except Exception as e:
        print(f"An error occurred: {e}")
        return f"å¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯,è¯·ç¨åå†è¯•ã€‚é”™è¯¯å†…å®¹ä¸ºï¼š{e}", "", ""
    finally:
        # ä¸å†åœ¨è¿™é‡Œå…³é—­æ¸¸æ ‡å’Œè¿æ¥
        pass

    return answer, highlighted_document, full_history


def clear_context(user_id):
    """
    æ¸…é™¤å¯¹è¯å†å²
    """
    if user_id in conversation_history:
        conversation_history[user_id] = []
    return "", "", ""


if __name__ == "__main__":
    with gr.Blocks(title=title, theme='ParityError/Interstellar') as zero_pal:
        gr.Markdown(title_markdown)

        with gr.Row():
            with gr.Column():
                user_id = gr.Textbox(
                    placeholder="è¯·è¾“å…¥æ‚¨çš„çœŸå®å§“åæˆ–æ˜µç§°ä½œä¸ºç”¨æˆ·ID(Please enter your real name or nickname as the user ID.)",
                    label="ç”¨æˆ·ID(Username)")
                inputs = gr.Textbox(
                    placeholder="è¯·æ‚¨åœ¨è¿™é‡Œè¾“å…¥ä»»ä½•å…³äº LightZero çš„é—®é¢˜ã€‚(Please enter any questions about LightZero here.)",
                    label="é—®é¢˜(Question)")
                # temperature = gr.Slider(minimum=0.0, maximum=1.0, value=0.01, step=0.01, label="æ¸©åº¦å‚æ•°")
                k = gr.Slider(minimum=1, maximum=7, value=3, step=1,
                              label="æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å—çš„æ•°é‡(The number of relevant document blocks retrieved.)")  # readmeæ€»é•¿åº¦ä¸º35000å·¦å³ï¼Œæ–‡æ®µå—é•¿åº¦ä¸º5000ï¼Œå› æ­¤æœ€å¤§å€¼ä¸º35000/5000=7
                with gr.Row():
                    gr_submit = gr.Button('æäº¤(Submit)')
                    gr_clear = gr.Button('æ¸…é™¤å¯¹è¯å†å²(Clear Context)')

            outputs_answer = gr.Textbox(
                placeholder="å½“ä½ ç‚¹å‡»æäº¤æŒ‰é’®å,è¿™é‡Œä¼šæ˜¾ç¤º RAG æ¨¡å‹ç»™å‡ºçš„å›ç­”ã€‚ï¼ˆAfter you click the submit button, the answer given by the RAG model will be displayed here.ï¼‰",
                label="å›ç­”(Answer)")
        outputs_history = gr.Textbox(label="å¯¹è¯å†å²(Conversation History)")
        with gr.Row():
            outputs_context = gr.Markdown(
                label="å‚è€ƒçš„æ–‡æ¡£(æ£€ç´¢å¾—åˆ°çš„ç›¸å…³æ–‡æ®µç”¨é«˜äº®æ˜¾ç¤º) Referenced documents (the relevant excerpts retrieved are highlighted).")
        gr_clear.click(clear_context, inputs=user_id, outputs=[outputs_context, outputs_history])
        gr_submit.click(
            rag_answer,
            # inputs=[inputs, temperature, k, user_id],
            inputs=[inputs, k, user_id],
            outputs=[outputs_answer, outputs_context, outputs_history],
        )
        gr.Markdown(tos_markdown)

    concurrency = int(os.environ.get('CONCURRENCY', os.cpu_count()))
    favicon_path = os.path.join(os.path.dirname(__file__), 'assets', 'avatar.png')
    zero_pal.queue().launch(max_threads=concurrency, favicon_path=favicon_path, share=True)

    # åœ¨åˆé€‚çš„åœ°æ–¹,ä¾‹å¦‚ç¨‹åºé€€å‡ºæ—¶,è°ƒç”¨close_db_connectionå‡½æ•°
    close_db_connection()
