import os

import gradio as gr
from dotenv import load_dotenv
from langchain.document_loaders import TextLoader

from rag_demo import load_and_split_document, create_vector_store, setup_rag_chain, execute_query

# ç¯å¢ƒè®¾ç½®
load_dotenv()  # åŠ è½½ç¯å¢ƒå˜é‡
QUESTION_LANG = os.getenv("QUESTION_LANG")  # ä»ç¯å¢ƒå˜é‡è·å– QUESTION_LANG
assert QUESTION_LANG in ['cn', 'en'], QUESTION_LANG

if QUESTION_LANG == "cn":
    title = "LightZero RAG Demo"
    title_markdown = """
    <div align="center">
        <img src="https://raw.githubusercontent.com/puyuan1996/RAG/main/assets/banner.svg" width="80%" height="20%" alt="Banner Image">
    </div>
    <h2 style="text-align: center; color: black;"><a href="https://github.com/puyuan1996/RAG"> LightZero RAG Demo</a></h2>
    <h4 align="center"> ğŸ“¢è¯´æ˜ï¼šè¯·æ‚¨åœ¨ä¸‹é¢çš„"é—®é¢˜ï¼ˆQï¼‰"æ¡†ä¸­è¾“å…¥ä»»ä½•å…³äº LightZero çš„é—®é¢˜ï¼Œç„¶åç‚¹å‡»"æäº¤"æŒ‰é’®ã€‚å³ä¾§"å›ç­”ï¼ˆAï¼‰"æ¡†ä¸­ä¼šæ˜¾ç¤º RAG æ¨¡å‹ç»™å‡ºçš„å›ç­”ã€‚åœ¨ QA æ çš„ä¸‹æ–¹ä¼šç»™å‡ºå‚è€ƒæ–‡æ¡£ï¼ˆå…¶ä¸­æ£€ç´¢å¾—åˆ°çš„ç›¸å…³æ–‡æ®µä¼šç”¨é»„è‰²é«˜äº®æ˜¾ç¤ºï¼‰ã€‚</h4>
    <h4 align="center"> å¦‚æœä½ å–œæ¬¢è¿™ä¸ªé¡¹ç›®ï¼Œè¯·ç»™æˆ‘ä»¬åœ¨ GitHub ç‚¹ä¸ª star âœ¨ ã€‚æˆ‘ä»¬å°†ä¼šæŒç»­ä¿æŒæ›´æ–°ã€‚  </h4>
    <strong><h5 align="center">æ³¨æ„ï¼šç®—æ³•æ¨¡å‹çš„è¾“å‡ºå¯èƒ½åŒ…å«ä¸€å®šçš„éšæœºæ€§ã€‚ç›¸å…³ç»“æœä¸ä»£è¡¨ä»»ä½•å¼€å‘è€…å’Œç›¸å…³ AI æœåŠ¡çš„æ€åº¦å’Œæ„è§ã€‚æœ¬é¡¹ç›®å¼€å‘è€…ä¸å¯¹ç”Ÿæˆç»“æœä½œä»»ä½•ä¿è¯ï¼Œä»…ä¾›å‚è€ƒã€‚<h5></strong>
    """
    tos_markdown = """
    ### ä½¿ç”¨æ¡æ¬¾
    ç©å®¶ä½¿ç”¨æœ¬æœåŠ¡é¡»åŒæ„ä»¥ä¸‹æ¡æ¬¾ï¼š
    è¯¥æœåŠ¡æ˜¯ä¸€é¡¹æ¢ç´¢æ€§ç ”ç©¶é¢„è§ˆç‰ˆï¼Œä»…ä¾›éå•†ä¸šç”¨é€”ã€‚å®ƒä»…æä¾›æœ‰é™çš„å®‰å…¨æªæ–½ï¼Œå¹¶å¯èƒ½ç”Ÿæˆä»¤äººåæ„Ÿçš„å†…å®¹ã€‚ä¸å¾—å°†å…¶ç”¨äºä»»ä½•éæ³•ã€æœ‰å®³ã€æš´åŠ›ã€ç§æ—ä¸»ä¹‰ç­‰ç›®çš„ã€‚
    å¦‚æœæ‚¨çš„æ¸¸ç©ä½“éªŒæœ‰ä¸ä½³ä¹‹å¤„ï¼Œè¯·å‘é€é‚®ä»¶è‡³ opendilab@pjlab.org.cn ï¼ æˆ‘ä»¬å°†åˆ é™¤ç›¸å…³ä¿¡æ¯ï¼Œå¹¶ä¸æ–­æ”¹è¿›è¿™ä¸ªé¡¹ç›®ã€‚
    ä¸ºäº†è·å¾—æœ€ä½³ä½“éªŒï¼Œè¯·ä½¿ç”¨å°å¼ç”µè„‘ï¼Œå› ä¸ºç§»åŠ¨è®¾å¤‡å¯èƒ½ä¼šå½±å“å¯è§†åŒ–æ•ˆæœã€‚
    **ç‰ˆæƒæ‰€æœ‰ 2024 OpenDILabã€‚**
    """

# è·¯å¾„å˜é‡ï¼Œæ–¹ä¾¿ä¹‹åçš„æ–‡ä»¶ä½¿ç”¨
file_path = './documents/LightZero_README.zh.md'

# åŠ è½½åŸå§‹Markdownæ–‡æ¡£
loader = TextLoader(file_path)
orig_documents = loader.load()

def rag_answer(question, model_name, temperature, embedding_model, k):
    """
    å¤„ç†ç”¨æˆ·é—®é¢˜å¹¶è¿”å›ç­”æ¡ˆå’Œé«˜äº®æ˜¾ç¤ºçš„ä¸Šä¸‹æ–‡

    :param question: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
    :param model_name: ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹åç§°
    :param temperature: ç”Ÿæˆç­”æ¡ˆæ—¶ä½¿ç”¨çš„æ¸©åº¦å‚æ•°
    :param embedding_model: ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹
    :param k: æ£€ç´¢åˆ°çš„æ–‡æ¡£å—æ•°é‡
    :return: æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆå’Œé«˜äº®æ˜¾ç¤ºä¸Šä¸‹æ–‡çš„Markdownæ–‡æœ¬
    """
    try:
        chunks = load_and_split_document(file_path)
        retriever = create_vector_store(chunks, model=embedding_model, k=k)
        rag_chain = setup_rag_chain(model_name=model_name, temperature=temperature)

        retrieved_documents, answer = execute_query(retriever, rag_chain, question, model_name=model_name, temperature=temperature)
        # åœ¨æ–‡æ¡£ä¸­é«˜äº®æ˜¾ç¤ºä¸Šä¸‹æ–‡
        context = [retrieved_documents[i].page_content for i in range(len(retrieved_documents))]
        highlighted_document = orig_documents[0].page_content
        for i in range(len(context)):
            highlighted_document = highlighted_document.replace(context[i], f"<mark>{context[i]}</mark>")
    except Exception as e:
        print(f"An error occurred: {e}")
        return "å¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚", ""
    return answer, highlighted_document


if __name__ == "__main__":
    with gr.Blocks(title=title, theme='ParityError/Interstellar') as rag_demo:
        gr.Markdown(title_markdown)

        with gr.Row():
            with gr.Column():
                inputs = gr.Textbox(
                    placeholder="è¯·æ‚¨è¾“å…¥ä»»ä½•å…³äº LightZero çš„é—®é¢˜ã€‚",
                    label="é—®é¢˜ (Q)")
                model_name = gr.Dropdown(
                    choices=['abab6-chat', 'glm-4', 'gpt-3.5-turbo', 'gpt-4', 'gpt-4-turbo'],
                    value='abab6-chat',
                    label="é€‰æ‹©è¯­è¨€æ¨¡å‹")
                temperature = gr.Slider(minimum=0.0, maximum=1.0, value=0.01, step=0.01, label="æ¸©åº¦å‚æ•°")
                embedding_model = gr.Dropdown(
                    choices=['HuggingFace', 'TensorflowHub', 'OpenAI'],
                    value='HuggingFace',
                    label="é€‰æ‹©åµŒå…¥æ¨¡å‹")
                k = gr.Slider(minimum=1, maximum=10, value=5, step=1, label="æ£€ç´¢åˆ°çš„æ–‡æ¡£å—æ•°é‡")
                gr_submit = gr.Button('æäº¤')

            outputs_answer = gr.Textbox(placeholder="å½“ä½ ç‚¹å‡»æäº¤æŒ‰é’®åï¼Œè¿™é‡Œä¼šæ˜¾ç¤º RAG æ¨¡å‹ç»™å‡ºçš„å›ç­”ã€‚",
                                        label="å›ç­” (A)")
        with gr.Row():
            outputs_context = gr.Markdown(label="å‚è€ƒçš„æ–‡æ¡£ï¼Œæ£€ç´¢å¾—åˆ°çš„ context ç”¨é«˜äº®æ˜¾ç¤º (C)")

        gr.Markdown(tos_markdown)

        gr_submit.click(
            rag_answer,
            inputs=[inputs, model_name, temperature, embedding_model, k],
            outputs=[outputs_answer, outputs_context],
        )

    concurrency = int(os.environ.get('CONCURRENCY', os.cpu_count()))
    favicon_path = os.path.join(os.path.dirname(__file__), 'assets', 'avatar.png')
    rag_demo.queue().launch(max_threads=concurrency, favicon_path=favicon_path, share=True)